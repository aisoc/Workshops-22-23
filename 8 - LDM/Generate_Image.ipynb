{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPTextModel, CLIPTokenizer, CLIPFeatureExtractor, CLIPProcessor\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "\n",
    "from src import StableDiff\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "\n",
    "from diffusers import LMSDiscreteScheduler\n",
    "from tqdm.auto import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "#Setup PyTorch\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(StableDiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Init CLIP tokenizer and model\n",
    "model_path_clip = \"openai/clip-vit-large-patch14\"\n",
    "auth_token = \"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\" #TODO: Replace this with huggingface auth token as a string if model is not already downloaded\n",
    "clip = StableDiff.CLIP(device,\n",
    "                       clip_model=CLIPModel.from_pretrained(model_path_clip, torch_dtype=torch.float16, use_auth_token=auth_token),\n",
    "                       clip_processor=CLIPProcessor.from_pretrained(model_path_clip, use_auth_token=auth_token)\n",
    "                       )\n",
    "clip.load()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Init diffusion model\n",
    "model_path_diffusion = \"runwayml/stable-diffusion-v1-5\"\n",
    "stable_diff = StableDiff.SimpleDiff(device,\n",
    "                                    unet=UNet2DConditionModel.from_pretrained(model_path_diffusion, subfolder=\"unet\", use_auth_token=auth_token, torch_dtype=torch.float16),\n",
    "                                    vae=AutoencoderKL.from_pretrained(model_path_diffusion, subfolder=\"vae\", use_auth_token=auth_token, torch_dtype=torch.float16))\n",
    "stable_diff.load()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Image Generation from Prompt\n",
    "prompt = \"Cartoon of a Student in Southampton\"\n",
    "\n",
    "#Perform CLIP embeddings\n",
    "with autocast(\"cuda\"):\n",
    "    embedding_unconditional = clip.embed_text(\"\")\n",
    "    embedding_conditional = clip.embed_text(prompt)\n",
    "\n",
    "    print(embedding_unconditional.shape)\n",
    "    print(embedding_conditional.shape)\n",
    "\n",
    "stable_diff.generate(embedding_unconditional=embedding_unconditional,\n",
    "                     embedding_conditional=embedding_conditional,\n",
    "                     tokens_length=clip.max_length, seed=1234, guidance_scale=8.0, steps=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Image Variations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "init_image = Image.open(\"polar_bear.png\")\n",
    "\n",
    "#Perform CLIP embeddings\n",
    "with autocast(\"cuda\"):\n",
    "    embedding_unconditional = clip.embed_text(\"\")\n",
    "    embedding_image = clip.embed_image(init_image)\n",
    "\n",
    "    # inputs = clip.clip_processor(images=init_image, return_tensors=\"pt\")\n",
    "    # inputs.to(clip.device)\n",
    "    # print(inputs.keys())\n",
    "    # # return self.clip_model.visual_projection(self.clip_model.vision_model(**inputs)[1]) #TODO\n",
    "    # embedding_image_raw = clip.clip_model.vision_model(pixel_values=inputs.pixel_values)\n",
    "    # print(embedding_image_raw.keys())\n",
    "    # print(\"TEST\", embedding_image_raw.last_hidden_state[:,:77].shape)\n",
    "    # embedding_image = clip.clip_model.visual_projection(embedding_image_raw.last_hidden_state[:,:77])\n",
    "\n",
    "    print(embedding_unconditional.shape)\n",
    "    print(embedding_image.shape)\n",
    "\n",
    "stable_diff.generate(embedding_unconditional=embedding_unconditional,\n",
    "                     embedding_conditional=embedding_image,\n",
    "                     tokens_length=clip.max_length, seed=123, guidance_scale=6.0, steps=100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}