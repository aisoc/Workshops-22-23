{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"csv_agile_H_Southern_England.csv\", header=None)\n",
    "df = df.rename(columns={\n",
    "    0:\"date\",\n",
    "    1:\"time\",\n",
    "    4:\"cost\"\n",
    "})\n",
    "df = df[[\"date\",\"time\", \"cost\"]]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Training dataset\n",
    "df_train = df[df.date<='2022-01-01'] #before 2022\n",
    "#Test dataset\n",
    "df_test =  df[df.date>'2022-01-01'] # after 2022\n",
    "\n",
    "# print(\"test\", len(df_test) / (len(df_train)+len(df_train)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-process"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "length = 128 #length of sequence\n",
    "\n",
    "#Sliding Window\n",
    "def sliding_window(array):\n",
    "    window_step = 1\n",
    "    window_size = 128 #2.6 days\n",
    "    return np.lib.stride_tricks.sliding_window_view(array, window_size)\n",
    "\n",
    "train = sliding_window(df_train[\"cost\"].to_numpy())\n",
    "test = sliding_window(df_test[\"cost\"].to_numpy())\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define RNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Define RNN Model\n",
    "#https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "hidden_size = 5\n",
    "num_layers = 2\n",
    "activation = \"relu\" #relu or tanh\n",
    "bidirectional = False\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.rnn = torch.nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=0,\n",
    "            nonlinearity=activation,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=False\n",
    "        )\n",
    "        # compress output to the same dim as y\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden_prev):\n",
    "        out, hidden_prev = self.rnn(x, hidden_prev) # [1, seq, h] => [seq, h]  (batch=1)\n",
    "        out = out.reshape(-1, hidden_size)  # stack batch and seq\n",
    "\n",
    "        # linear layer so that output is not [seq,h] but [seq, 1]\n",
    "        # so it is comparable with y, for loss calculation\n",
    "        out = self.linear(out)  # [seq, h] => [seq, 1]\n",
    "        out = out.unsqueeze(dim=0)  # => [1, seq, 1]\n",
    "        return out, hidden_prev\n",
    "\n",
    "rnn = Net()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n",
    "\n",
    "# rnn= torch.nn.RNN(input_size=input_size, hidden_size=hidden_size,num_layers=num_layers,\n",
    "#                   dropout=0, nonlinearity=\"relu\", bidirectional=bidirectional)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 1 #number of batches for training\n",
    "\n",
    "def train_rnn(rnn, n_steps, print_every):\n",
    "\n",
    "    # initialize the hidden state\n",
    "    if bidirectional:\n",
    "        hidden = torch.zeros(2*num_layers,batch_size,hidden_size)\n",
    "    else:\n",
    "        hidden = torch.zeros(num_layers,batch_size,hidden_size)\n",
    "\n",
    "    for batch_i, step in enumerate(range(n_steps)):\n",
    "        # ; y: 49 points 1-50\n",
    "        data=train[batch_i]\n",
    "        print(data[:-1].shape)\n",
    "        break\n",
    "        x = torch.tensor(data[:-1]).float().reshape(length - 1, batch_size, input_size) # x: 127 points 0-126 [length,batch_size,input_size]\n",
    "        # x = torch.randn(length-1,batch_size,input_size)\n",
    "        y = torch.tensor(data[1:]).float().reshape(length - 1, batch_size, input_size)  # y: 127 points 1-127  [length,batch_size,output_size]\n",
    "\n",
    "        # outputs from the rnn\n",
    "        prediction, hidden = rnn(x, hidden)\n",
    "        # prediction = x\n",
    "\n",
    "        # Representing Memory #\n",
    "        hidden = hidden.data # make a new variable for hidden and detach the hidden state from its history\n",
    "                            # this way, we don't backpropagate through the entire history\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(prediction, y)\n",
    "\n",
    "        optimizer.zero_grad()# zero gradients: maps 0 to None to save memory\n",
    "\n",
    "        # perform backprop and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # display loss and predictions\n",
    "        if batch_i%print_every == 0:\n",
    "            print('Loss: ', loss.item())\n",
    "    return rnn\n",
    "\n",
    "rnn = train_rnn(rnn, 2000, 10)\n",
    "\n",
    "print(rnn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if bidirectional:\n",
    "    hidden = torch.zeros(2*num_layers,batch_size,hidden_size)\n",
    "else:\n",
    "    hidden = torch.zeros(num_layers,batch_size,hidden_size)\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "for batch_i in range(len(test)):#\n",
    "    data=test[batch_i]\n",
    "    x = torch.tensor(data[:-1]).float().reshape(length - 1, batch_size, input_size)  # [seq_len, b, fea_len]\n",
    "    y = torch.tensor(data[1:]).float().reshape(length - 1, batch_size, input_size)  # [seq_len, b, fea_len]\n",
    "\n",
    "    prediction, hidden = rnn(x, hidden)\n",
    "    hidden = hidden.data\n",
    "    total_loss += criterion(prediction, y)\n",
    "\n",
    "print(total_loss / len(test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,127,127), data[:-1], label=\"Test\")\n",
    "plt.plot(np.linspace(0,127,127), prediction.detach().reshape(127), label=\"Prediction\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}